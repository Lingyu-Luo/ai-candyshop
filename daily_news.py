import os
import json
import requests
import feedparser
import datetime

# è¯·ç¡®ä¿ç¯å¢ƒå˜é‡ä¸­æœ‰ SILICONFLOW_API_KEYï¼Œæˆ–è€…ç›´æ¥å¡«åœ¨è¿™é‡Œ
API_KEY = os.getenv("SILICONFLOW_API_KEY")
BASE_URL = "https://api.siliconflow.cn/v1/chat/completions"
MODEL_NAME = "deepseek-ai/DeepSeek-V3.2"
KEYWORDS = ["LLM", "Transformer", "GPT", "Claude", "Gemini", "DeepSeek",
            "RAG", "Agent", "Diffusion", "Quantization", "MoE"]


def fetch_hackernews():
    """æŠ“å– Hacker News å‰ 50 æ¡ä¸­çš„ AI ç›¸å…³æ–°é—»"""
    print("ğŸ“¡ æ­£åœ¨æŠ“å– Hacker News...")
    try:
        top_ids = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json").json()[:50]
        news_list = []
        for pid in top_ids:
            item = requests.get(f"https://hacker-news.firebaseio.com/v0/item/{pid}.json").json()
            if not item or 'title' not in item: continue

            # ç®€å•çš„å…³é”®è¯åŒ¹é…
            if any(k.lower() in item['title'].lower() for k in KEYWORDS):
                news_list.append({
                    "source": "Hacker News",
                    "title": item['title'],
                    "url": item.get('url', f"https://news.ycombinator.com/item?id={pid}"),
                    "desc": f"Score: {item.get('score', 0)}"
                })
        return news_list
    except Exception as e:
        print(f"âŒ Hacker News æŠ“å–å¤±è´¥: {e}")
        return []


def fetch_arxiv():
    """æŠ“å– ArXiv (cs.CL è®¡ç®—è¯­è¨€å­¦) æœ€æ–°è®ºæ–‡"""
    print("ğŸ“¡ æ­£åœ¨æŠ“å– ArXiv (cs.CL)...")
    try:
        url = 'http://export.arxiv.org/api/query?search_query=cat:cs.CL&start=0&max_results=10&sortBy=submittedDate&sortOrder=descending'
        feed = feedparser.parse(url)
        papers = []
        for entry in feed.entries:
            # ä»…ä¿ç•™æ‘˜è¦ä¸­åŒ…å«å…³é”®è¯çš„è®ºæ–‡ï¼Œæˆ–è€…æ— æ¡ä»¶ä¿ç•™å‰5ç¯‡
            if any(k.lower() in entry.title.lower() for k in KEYWORDS):
                papers.append({
                    "source": "ArXiv",
                    "title": entry.title.replace('\n', ' '),
                    "url": entry.link,
                    "desc": entry.summary[:150] + "..."  # åªå–æ‘˜è¦å‰150å­—
                })
        return papers
    except Exception as e:
        print(f"âŒ ArXiv æŠ“å–å¤±è´¥: {e}")
        return []


def fetch_huggingface_daily():
    """æŠ“å– Hugging Face Daily Papers (çƒ­é—¨è®ºæ–‡)"""
    print("ğŸ“¡ æ­£åœ¨æŠ“å– Hugging Face Daily Papers...")
    try:
        # ä½¿ç”¨ Hugging Face çš„å…¬å¼€ API
        resp = requests.get("https://huggingface.co/api/daily_papers")
        if resp.status_code != 200:
            return []

        data = resp.json()
        papers = []
        # è·å–ä»Šå¤©çš„çƒ­é—¨è®ºæ–‡ï¼ˆAPIè¿”å›çš„æ˜¯åˆ—è¡¨ï¼‰
        for item in data[:8]:  # å–å‰8ç¯‡
            paper = item['paper']
            papers.append({
                "source": "Hugging Face",
                "title": paper['title'],
                "url": f"https://huggingface.co/papers/{paper['id']}",
                "desc": f"Votes: {item.get('numComments', 0)} | {paper['summary'][:100] if 'summary' in paper else 'No summary'}"
            })
        return papers
    except Exception as e:
        print(f"âŒ Hugging Face æŠ“å–å¤±è´¥: {e}")
        return []


def chat_with_llm_stream(model: str, prompt: str):
    """
    è°ƒç”¨ SiliconFlow API è¿›è¡Œæ€»ç»“ (åŸºäºä½ æä¾›çš„ä»£ç ä¿®æ”¹)
    è¿”å›å®Œæ•´çš„æ€»ç»“æ–‡æœ¬å­—ç¬¦ä¸²ã€‚
    """
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }

    messages = [{"role": "user", "content": prompt}]

    payload = {
        "model": model,
        "messages": messages,
        "max_tokens": 163840,
        "temperature": 0.5,
        "enable_thinking": True,
        "stream": True
    }

    print(f"\nğŸ§  æ­£åœ¨è°ƒç”¨ {model} è¿›è¡Œæ·±åº¦æ€»ç»“...\n")
    print("-" * 40)

    try:
        response = requests.post(BASE_URL, json=payload, headers=headers, stream=True)
        response.raise_for_status()

        full_content = ""
        full_reasoning = ""

        for chunk in response.iter_lines():
            if chunk:
                chunk_str = chunk.decode('utf-8').replace('data: ', '')
                if chunk_str == "[DONE]": break

                try:
                    chunk_data = json.loads(chunk_str)
                    delta = chunk_data.get('choices', [{}])[0].get('delta', {})

                    # å¤„ç†æ€ç»´é“¾ (å¦‚æœä½ ç”¨ R1)
                    if 'reasoning_content' in delta and delta['reasoning_content']:
                        print(delta['reasoning_content'], end="", flush=True)  # æ‰“å°æ€è€ƒè¿‡ç¨‹
                        full_reasoning += delta['reasoning_content']

                    # å¤„ç†æ­£æ–‡
                    if 'content' in delta and delta['content']:
                        content = delta['content']
                        print(content, end="", flush=True)  # å®æ—¶æ‰“å°æ­£æ–‡
                        full_content += content

                except json.JSONDecodeError:
                    continue

        print("\n" + "-" * 40 + "\n")
        return full_content

    except Exception as e:
        print(f"\nâŒ API è°ƒç”¨å‡ºé”™: {e}")
        return "AI æ€»ç»“å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API Key æˆ–ç½‘ç»œè¿æ¥ã€‚"


def generate_daily_report():
    # 1. è·å–æ•°æ®
    news_items = []
    news_items.extend(fetch_huggingface_daily())  # ä¼˜å…ˆçœ‹ HF è®ºæ–‡
    news_items.extend(fetch_hackernews())
    news_items.extend(fetch_arxiv())

    if not news_items:
        print("ğŸ˜´ ä»Šå¤©å¥½åƒæ²¡ä»€ä¹ˆå¤§æ–°é—»ã€‚")
        return

    # 2. æ„é€  Prompt
    # å°†æ–°é—»åˆ—è¡¨è½¬æ¢ä¸ºæ–‡æœ¬å—
    news_context = ""
    for idx, item in enumerate(news_items, 1):
        news_context += f"{idx}. [{item['source']}] {item['title']}\n   Link: {item['url']}\n   Info: {item['desc']}\n\n"

    prompt = f"""
    ä½ æ˜¯ä¸€ä½æå…¶ä¸“ä¸šã€çœ¼å…‰ç‹¬åˆ°çš„ AI æŠ€æœ¯æ—¥æŠ¥ä¸»ç¼–ã€‚
    è¯·é˜…è¯»ä»¥ä¸‹ä»Šå¤©æŠ“å–åˆ°çš„åŸå§‹ AI èµ„è®¯/è®ºæ–‡åˆ—è¡¨ï¼š

    {news_context}

    è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼Œç”Ÿæˆä¸€ä»½é«˜è´¨é‡çš„ Markdown æ—¥æŠ¥ï¼š

    1. **ç­›é€‰ä¸å»é‡**ï¼šä»åˆ—è¡¨ä¸­æŒ‘é€‰å‡ºæœ€é‡è¦ã€æœ€å…·æŠ€æœ¯ä»·å€¼çš„ 5-8 æ¡æ–°é—»/è®ºæ–‡ã€‚å¿½ç•¥åŒè´¨åŒ–ä¸¥é‡æˆ–æ— æ„ä¹‰çš„å†…å®¹ã€‚
    2. **ä¸­æ–‡æ·±åº¦ç‚¹è¯„**ï¼š
       - å°†æ ‡é¢˜ç¿»è¯‘ä¸ºä¸­æ–‡ã€‚
       - ä¸ºæ¯ä¸€æ¡å†™ä¸€æ®µç®€çŸ­ä½†æ·±åˆ»çš„ç‚¹è¯„ï¼ˆ2-3å¥è¯ï¼‰ã€‚ä¸è¦åªå¤è¿°æ‘˜è¦ï¼Œè¦æŒ‡å‡ºå®ƒçš„æŠ€æœ¯åˆ›æ–°ç‚¹ã€è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Œæˆ–è€…å¯¹è¡Œä¸šæ„å‘³ç€ä»€ä¹ˆã€‚
    3. **åˆ†ç±»å±•ç¤º**ï¼šè¯·æŒ‰ä»¥ä¸‹ç±»åˆ«åˆ†ç±»ï¼š
       - ğŸ”¥ **é‡ç£…å¤´æ¡** (Must Read)
       - ğŸ“ **ç¡¬æ ¸è®ºæ–‡** (Research)
       - ğŸ› ï¸ **å¼€æº/å·¥å…·** (Engineering)
    4. **æ ¼å¼è¦æ±‚**ï¼šä½¿ç”¨ Markdown æ ¼å¼ï¼ŒåŒ…å«åŸæ–‡é“¾æ¥ã€‚

    è¾“å‡ºé£æ ¼è¦å¹²ç»ƒã€æå®¢ï¼Œæ‹’ç»åºŸè¯ã€‚
    """

    # 3. AI å¤„ç†
    report_content = chat_with_llm_stream(MODEL_NAME, prompt)

    # 4. ä¿å­˜æ–‡ä»¶
    output_dir = "./DailyNews/"
    os.makedirs(output_dir, exist_ok=True)
    today_str = datetime.date.today().isoformat()
    filename = os.path.join(output_dir, f"AI_Daily_Report_{today_str}.md")

    with open(filename, "w", encoding="utf-8") as f:
        f.write(f"# ğŸ¤– AI æ¯æ—¥æ·±åº¦ç®€æŠ¥ ({today_str})\n\n")
        f.write(f"> ç”± {MODEL_NAME} è‡ªåŠ¨ç”Ÿæˆ\n\n")
        f.write(report_content)
        f.write("\n\n---\n")
        f.write("### ğŸ”— åŸå§‹èµ„è®¯æ•°æ®æº\n")
        f.write(f"å…±æŠ“å– {len(news_items)} æ¡åŸå§‹æ•°æ®ï¼Œç²¾é€‰å¦‚ä¸Šã€‚")

    print(f"âœ… æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ï¼æ–‡ä»¶å·²ä¿å­˜ä¸º: {filename}")


if __name__ == "__main__":
    generate_daily_report()