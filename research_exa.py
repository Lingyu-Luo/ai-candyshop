import streamlit as st
from openai import OpenAI
import os
import json
import re
from datetime import datetime
from exa_py import Exa
import asyncio
import time
import logging

# é…ç½®åŸºç¡€ä¿¡æ¯
client = OpenAI(
    base_url='https://api.siliconflow.cn/v1/',
    api_key=os.getenv("SILICONFLOW_API_KEY")
)

# Exa API é…ç½®
exa = Exa(os.getenv("EXA_API_KEY"))

RESEARCH_DIR = "output/DeepResearch"
os.makedirs(RESEARCH_DIR, exist_ok=True)

# æ¨¡å‹é…ç½®
RESEARCH_MODEL = "Pro/zai-org/GLM-4.7"
ANALYSIS_MODEL = "Pro/zai-org/GLM-4.7"

# Token é…ç½®å‚æ•°
RESEARCH_MAX_TOKENS = 16384  # ç ”ç©¶æ¨¡å‹çš„æœ€å¤§tokenæ•°
ANALYSIS_MAX_TOKENS = 163840  # åˆ†ææ¨¡å‹çš„æœ€å¤§tokenæ•°

def setup_logging():
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = os.path.join(RESEARCH_DIR, "logs")
    os.makedirs(log_dir, exist_ok=True)

    # é…ç½®æ—¥å¿—æ ¼å¼
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'

    # è®¾ç½®æ–‡ä»¶æ—¥å¿—
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"research_{timestamp}.log")

    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()  # æ§åˆ¶å°è¾“å‡º
        ]
    )

    return logging.getLogger("DeepResearch")

logger = setup_logging()

def init_research_session():
    """åˆå§‹åŒ–ç ”ç©¶ä¼šè¯çŠ¶æ€"""
    if 'research_query' not in st.session_state:
        st.session_state.research_query = ""
    if 'research_steps' not in st.session_state:
        st.session_state.research_steps = []
    if 'current_research' not in st.session_state:
        st.session_state.current_research = None
    if 'research_depth' not in st.session_state:
        st.session_state.research_depth = 3
    if 'max_sources_per_step' not in st.session_state:
        st.session_state.max_sources_per_step = 5
    if 'research_in_progress' not in st.session_state:
        st.session_state.research_in_progress = False


def extract_json_from_response(content: str, default=None):
    """
    ä» LLM çš„å›å¤æ–‡æœ¬ä¸­é²æ£’åœ°æå– JSON å¯¹è±¡ã€‚

    ç­–ç•¥ï¼š
    1. å°è¯•ç›´æ¥è§£æã€‚
    2. å°è¯•æå– Markdown ä»£ç å— (```json ... ```)ã€‚
    3. å°è¯•æš´åŠ›æŸ¥æ‰¾æœ€å¤–å±‚çš„ {} æˆ– [] ç»“æ„ã€‚

    Args:
        content (str): LLM è¿”å›çš„åŸå§‹å­—ç¬¦ä¸²ã€‚
        default (Any, optional): è§£æå¤±è´¥æ—¶çš„é»˜è®¤è¿”å›å€¼ã€‚é»˜è®¤ä¸º Noneã€‚

    Returns:
        dict | list | None: è§£æåçš„ JSON å¯¹è±¡ï¼Œå¤±è´¥åˆ™è¿”å› defaultã€‚
    """
    if not content:
        return default

    content = content.strip()

    # --- ç­–ç•¥ 1: ç›´æ¥å°è¯•è§£æ (æœ€å¿«) ---
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        pass

    # --- ç­–ç•¥ 2: æå– Markdown ä»£ç å— ---
    # åŒ¹é… ```json ... ``` æˆ– çº¯ ``` ... ```
    # re.DOTALL è®© . å¯ä»¥åŒ¹é…æ¢è¡Œç¬¦
    code_block_pattern = r"```(?:json)?\s*(.*?)\s*```"
    match = re.search(code_block_pattern, content, re.DOTALL)

    if match:
        json_str = match.group(1)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            # å¦‚æœä»£ç å—é‡Œä¸æ˜¯ JSONï¼Œç»§ç»­å°è¯•ç­–ç•¥ 3
            pass

    # --- ç­–ç•¥ 3: æš´åŠ›æŸ¥æ‰¾ JSON è¾¹ç•Œ ---
    # å¯»æ‰¾ç¬¬ä¸€ä¸ª '{' å’Œæœ€åä¸€ä¸ª '}' (é’ˆå¯¹ Object)
    # æˆ–è€… ç¬¬ä¸€ä¸ª '[' å’Œ æœ€åä¸€ä¸ª ']' (é’ˆå¯¹ Array)

    # æŸ¥æ‰¾ Object {}
    json_obj_match = re.search(r"(\{.*\})", content, re.DOTALL)
    if json_obj_match:
        try:
            return json.loads(json_obj_match.group(1))
        except json.JSONDecodeError:
            pass

    # æŸ¥æ‰¾ Array []
    json_arr_match = re.search(r"(\[.*\])", content, re.DOTALL)
    if json_arr_match:
        try:
            return json.loads(json_arr_match.group(1))
        except json.JSONDecodeError:
            pass

    # --- å¤±è´¥ ---
    logging.warning(f"JSONè§£æå¤±è´¥ï¼ŒåŸå§‹å†…å®¹å‰100å­—ç¬¦: {content[:100]}")
    return default


class ResearchStep:
    """ç ”ç©¶æ­¥éª¤ç±»"""

    def __init__(self, query, step_type, sources=None, analysis="", reasoning=""):
        self.query = query
        self.step_type = step_type  # 'search', 'analysis', 'synthesis'
        self.sources = sources or []
        self.analysis = analysis
        self.reasoning = reasoning
        self.timestamp = datetime.now().isoformat()

def generate_search_queries(main_query, existing_steps, depth_level):
    """ç”Ÿæˆæ·±åº¦æœç´¢æŸ¥è¯¢"""
    logger.info(f"ç”Ÿæˆæœç´¢æŸ¥è¯¢ - æ·±åº¦çº§åˆ«: {depth_level}, ä¸»è¦é—®é¢˜: {main_query[:100]}")

    context = ""
    if existing_steps:
        context = f"\nå·²å®Œæˆçš„ç ”ç©¶æ­¥éª¤ï¼š\n"
        for i, step in enumerate(existing_steps[-3:]):  # åªçœ‹æœ€è¿‘3æ­¥
            context += f"{i + 1}. {step.query} -> {step.analysis[:200]}...\n"
        logger.info(f"ä½¿ç”¨å·²æœ‰æ­¥éª¤ä¸Šä¸‹æ–‡ï¼Œæ­¥éª¤æ•°: {len(existing_steps[-3:])}")

    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åŠ©æ‰‹ã€‚åŸºäºä¸»è¦ç ”ç©¶é—®é¢˜å’Œå·²æœ‰è¿›å±•ï¼Œç”Ÿæˆ{st.session_state.max_sources_per_step}ä¸ªæ·±å…¥çš„æœç´¢æŸ¥è¯¢ã€‚

ä¸»è¦ç ”ç©¶é—®é¢˜ï¼š{main_query}
å½“å‰æ·±åº¦çº§åˆ«ï¼š{depth_level}/3
{context}

è¯·ç”Ÿæˆ{st.session_state.max_sources_per_step}ä¸ªä¸åŒè§’åº¦çš„æœç´¢æŸ¥è¯¢ï¼Œæ¯ä¸ªæŸ¥è¯¢åº”è¯¥ï¼š
1. é’ˆå¯¹é—®é¢˜çš„ä¸åŒæ–¹é¢
2. é¿å…é‡å¤å·²æœç´¢çš„å†…å®¹
3. é€æ­¥æ·±å…¥ç»†èŠ‚
4. åŒ…å«æœ€æ–°ä¿¡æ¯å’Œè¶‹åŠ¿

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{"queries": ["æŸ¥è¯¢1", "æŸ¥è¯¢2", "æŸ¥è¯¢3", "æŸ¥è¯¢4", "æŸ¥è¯¢5"]}}
"""

    try:
        logger.info("æ­£åœ¨è°ƒç”¨AIæ¨¡å‹ç”Ÿæˆæœç´¢æŸ¥è¯¢...")
        response = client.chat.completions.create(
            model=RESEARCH_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.6,
            max_tokens=1024
        )

        result = extract_json_from_response(response.choices[0].message.content.strip())
        queries = result.get("queries", [main_query])
        logger.info(f"æˆåŠŸç”Ÿæˆ {len(queries)} ä¸ªæœç´¢æŸ¥è¯¢: {queries}")
        return queries
    except Exception as e:
        logger.error(f"ç”Ÿæˆæœç´¢æŸ¥è¯¢å¤±è´¥: {str(e)}")
        return [main_query]


def search_with_exa(query, num_results=5):
    """ä½¿ç”¨Exa APIè¿›è¡Œæœç´¢"""
    logger.info(f"å¼€å§‹Exaæœç´¢ - æŸ¥è¯¢: {query}, ç»“æœæ•°: 10")
    try:
        search_result = exa.search_and_contents(
            query,
            include_domains=["arxiv.org", "nature.com", "science.org", "ieee.org", "acm.org"],
            text=True
        )

        sources = []
        for result in search_result.results:
            sources.append({
                "title": result.title,
                "url": result.url,
                "content": result.text[:2000] if result.text else "",
                "highlights": result.highlights[:3] if result.highlights else [],
                "published_date": getattr(result, 'published_date', None),
                "score": getattr(result, 'score', 0.0)
            })

        logger.info(f"Exaæœç´¢å®Œæˆï¼Œè·å¾— {len(sources)} ä¸ªæœ‰æ•ˆæ¥æº")
        return sources
    except Exception as e:
        logger.error(f"Exaæœç´¢å¤±è´¥: {str(e)}")
        st.error(f"Exaæœç´¢å¤±è´¥: {str(e)}")
        return []


def analyze_sources(query, sources, existing_context=""):
    """åˆ†ææœç´¢ç»“æœ"""
    logger.info(f"å¼€å§‹åˆ†ææ¥æº - æŸ¥è¯¢: {query}, æ¥æºæ•°: {len(sources)}")
    sources_text = ""
    for i, source in enumerate(sources):
        sources_text += f"\n--- æ¥æº {i + 1} ---\n"
        sources_text += f"æ ‡é¢˜: {source['title']}\n"
        sources_text += f"é“¾æ¥: {source['url']}\n"
        sources_text += f"å†…å®¹: {source['content']}\n"
        if source['highlights']:
            sources_text += f"é‡ç‚¹: {'; '.join(source['highlights'])}\n"

    logger.debug(f"å‡†å¤‡åˆ†æå†…å®¹é•¿åº¦: {len(sources_text)} å­—ç¬¦")

    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åˆ†æå¸ˆã€‚è¯·æ·±å…¥åˆ†æä»¥ä¸‹æœç´¢ç»“æœï¼Œé’ˆå¯¹æŸ¥è¯¢é—®é¢˜æä¾›è¯¦ç»†çš„åˆ†æã€‚

æŸ¥è¯¢é—®é¢˜ï¼š{query}

å·²æœ‰ç ”ç©¶èƒŒæ™¯ï¼š
{existing_context}

æœç´¢ç»“æœï¼š
{sources_text}

è¯·æä¾›ï¼š
1. å…³é”®å‘ç°å’Œæ´å¯Ÿ
2. ä¸åŒæ¥æºé—´çš„å…³è”å’Œå¯¹æ¯”
3. æ½œåœ¨çš„ç ”ç©¶æ–¹å‘
4. éœ€è¦è¿›ä¸€æ­¥æ¢ç´¢çš„é—®é¢˜
5. åŸºäºè¯æ®çš„ç»“è®º

è¯·ç»“æ„åŒ–è¾“å‡ºï¼Œä½¿ç”¨markdownæ ¼å¼ã€‚
"""

    try:
        logger.info("æ­£åœ¨è°ƒç”¨AIæ¨¡å‹è¿›è¡Œæ¥æºåˆ†æ...")
        stream = client.chat.completions.create(
            model=ANALYSIS_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=ANALYSIS_MAX_TOKENS,
            stream=True
        )

        full_analysis = ""
        full_reasoning = ""

        for chunk in stream:
            if chunk.choices[0].delta.content:
                full_analysis += chunk.choices[0].delta.content or ""

            if hasattr(chunk.choices[0].delta, 'reasoning_content'):
                reasoning = chunk.choices[0].delta.reasoning_content or ""
                full_reasoning += reasoning

        logger.info(f"åˆ†æå®Œæˆ - ç”Ÿæˆå†…å®¹é•¿åº¦: {len(full_analysis)} å­—ç¬¦ã€‚")
        return full_analysis, full_reasoning
    except Exception as e:
        logger.error(f"æ¥æºåˆ†æå¤±è´¥: {str(e)}")
        return f"åˆ†æå¤±è´¥: {str(e)}", ""


def analyze_sources_streaming(query, sources, existing_context="", placeholder=None):
    """åˆ†ææœç´¢ç»“æœ - æ”¯æŒæµå¼æ˜¾ç¤º"""
    logger.info(f"å¼€å§‹åˆ†ææ¥æº - æŸ¥è¯¢: {query}, æ¥æºæ•°: {len(sources)}")
    sources_text = ""
    for i, source in enumerate(sources):
        sources_text += f"\n--- æ¥æº {i + 1} ---\n"
        sources_text += f"æ ‡é¢˜: {source['title']}\n"
        sources_text += f"é“¾æ¥: {source['url']}\n"
        sources_text += f"å†…å®¹: {source['content']}\n"
        if source['highlights']:
            sources_text += f"é‡ç‚¹: {'; '.join(source['highlights'])}\n"

    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åˆ†æå¸ˆã€‚è¯·æ·±å…¥åˆ†æä»¥ä¸‹æœç´¢ç»“æœï¼Œé’ˆå¯¹æŸ¥è¯¢é—®é¢˜æä¾›è¯¦ç»†çš„åˆ†æã€‚

æŸ¥è¯¢é—®é¢˜ï¼š{query}

å·²æœ‰ç ”ç©¶èƒŒæ™¯ï¼š
{existing_context}

æœç´¢ç»“æœï¼š
{sources_text}

è¯·æä¾›ï¼š
1. å…³é”®å‘ç°å’Œæ´å¯Ÿ
2. ä¸åŒæ¥æºé—´çš„å…³è”å’Œå¯¹æ¯”
3. æ½œåœ¨çš„ç ”ç©¶æ–¹å‘
4. éœ€è¦è¿›ä¸€æ­¥æ¢ç´¢çš„é—®é¢˜
5. åŸºäºè¯æ®çš„ç»“è®º

è¯·ç»“æ„åŒ–è¾“å‡ºï¼Œä½¿ç”¨markdownæ ¼å¼ã€‚
"""

    try:
        logger.info("æ­£åœ¨è°ƒç”¨AIæ¨¡å‹è¿›è¡Œæ¥æºåˆ†æ...")
        stream = client.chat.completions.create(
            model=ANALYSIS_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=ANALYSIS_MAX_TOKENS,
            stream=True
        )

        full_analysis = ""
        full_reasoning = ""

        for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content or ""
                full_analysis += content

                # å®æ—¶æ›´æ–°æ˜¾ç¤º
                if placeholder:
                    placeholder.markdown(full_analysis + "â–Œ")

            if hasattr(chunk.choices[0].delta, 'reasoning_content'):
                reasoning = chunk.choices[0].delta.reasoning_content or ""
                full_reasoning += reasoning

        # ç§»é™¤å…‰æ ‡
        if placeholder:
            placeholder.markdown(full_analysis)

        logger.info(f"åˆ†æå®Œæˆ - ç”Ÿæˆå†…å®¹é•¿åº¦: {len(full_analysis)} å­—ç¬¦ã€‚")
        return full_analysis, full_reasoning
    except Exception as e:
        logger.error(f"æ¥æºåˆ†æå¤±è´¥: {str(e)}")
        return f"åˆ†æå¤±è´¥: {str(e)}", ""


def synthesize_research(main_query, all_steps, placeholder=None):
    """ç»¼åˆæ‰€æœ‰ç ”ç©¶ç»“æœ"""
    logger.info(f"å¼€å§‹ç»¼åˆç ”ç©¶ - ä¸»è¦é—®é¢˜: {main_query}, æ€»æ­¥éª¤æ•°: {len(all_steps)}")
    research_summary = ""
    for step in all_steps:
        research_summary += f"\n=== {step.query} ===\n"
        research_summary += f"ç±»å‹: {step.step_type}\n"
        research_summary += f"åˆ†æ: {step.analysis}\n"
        research_summary += f"æ¥æºæ•°é‡: {len(step.sources)}\n\n"

    logger.info(f"ç ”ç©¶æ‘˜è¦å‡†å¤‡å®Œæˆï¼Œæ€»é•¿åº¦: {len(research_summary)} å­—ç¬¦")

    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªé¡¶çº§ç ”ç©¶ä¸“å®¶ã€‚ï¿½ï¿½åŸºäºä»¥ä¸‹å®Œæ•´çš„æ·±åº¦ç ”ç©¶ç»“æœï¼Œä¸ºä¸»è¦ç ”ç©¶é—®é¢˜æä¾›comprehensive final reportã€‚

ä¸»è¦ç ”ç©¶é—®é¢˜ï¼š{main_query}

å®Œæ•´ç ”ç©¶è¿‡ç¨‹ï¼š
{research_summary}

è¯·æä¾›ä¸€ä»½ä¸“ä¸šçš„ç ”ç©¶æŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š

## æ‰§è¡Œæ‘˜è¦
- æ ¸å¿ƒå‘ç°
- ä¸»è¦ç»“è®º

## è¯¦ç»†åˆ†æ
- å…³é”®æ´å¯Ÿ
- è¶‹åŠ¿åˆ†æ
- æŠ€æœ¯ç»†èŠ‚

## å®è·µå»ºè®®
- å¯è¡Œçš„è§£å†³æ–¹æ¡ˆ
- å®æ–½å»ºè®®
- æ½œåœ¨é£é™©

## è¿›ä¸€æ­¥ç ”ç©¶æ–¹å‘
- æœªè§£å†³çš„é—®é¢˜
- ç ”ç©¶ç©ºç™½
- æœªæ¥æœºä¼š

## å‚è€ƒæ–‡çŒ®æ€»ç»“
- å…³é”®æ–‡çŒ®åˆ†ç±»
- å¯ä¿¡åº¦è¯„ä¼°

è¯·ä½¿ç”¨ä¸“ä¸šçš„å­¦æœ¯è¯­è¨€ï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°ã€ç»“ï¿½ï¿½å®Œæ•´ã€‚
"""

    try:
        logger.info("æ­£åœ¨ç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Š...")
        stream = client.chat.completions.create(
            model=ANALYSIS_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=ANALYSIS_MAX_TOKENS,
            stream=True
        )

        synthesis = ""
        reasoning = ""

        for chunk in stream:
            if chunk.choices[0].delta.content:
                synthesis += chunk.choices[0].delta.content or ""

                # å®æ—¶æ›´æ–°æ˜¾ç¤º
                if placeholder:
                    placeholder.markdown(synthesis + "â–Œ")

            if hasattr(chunk.choices[0].delta, 'reasoning_content'):
                reasoning += chunk.choices[0].delta.reasoning_content or ""

        # ç§»é™¤å…‰æ ‡
        if placeholder:
            placeholder.markdown(synthesis)

        logger.info(f"ç»¼åˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ - é•¿åº¦: {len(synthesis)} å­—ç¬¦ã€‚")
        return synthesis, reasoning
    except Exception as e:
        return f"ç»¼åˆåˆ†æå¤±è´¥: {str(e)}", ""


def synthesize_research_streaming(main_query, all_steps, placeholder=None):
    """ç»¼åˆæ‰€æœ‰ç ”ç©¶ç»“æœ - æ”¯æŒæµå¼æ˜¾ç¤º"""
    logger.info(f"å¼€å§‹ç»¼åˆç ”ç©¶ - ä¸»è¦é—®é¢˜: {main_query}, æ€»æ­¥éª¤æ•°: {len(all_steps)}")
    research_summary = ""
    for step in all_steps:
        research_summary += f"\n=== {step.query} ===\n"
        research_summary += f"ç±»å‹: {step.step_type}\n"
        research_summary += f"åˆ†æ: {step.analysis}\n"
        research_summary += f"æ¥æºæ•°é‡: {len(step.sources)}\n\n"

    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªé¡¶çº§ç ”ç©¶ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹å®Œæ•´çš„æ·±åº¦ç ”ç©¶ç»“æœï¼Œä¸ºä¸»è¦ç ”ç©¶é—®é¢˜æä¾›comprehensive final reportã€‚

ä¸»è¦ç ”ç©¶é—®é¢˜ï¼š{main_query}

å®Œæ•´ç ”ç©¶è¿‡ç¨‹ï¼š
{research_summary}

è¯·æä¾›ä¸€ä»½ä¸“ä¸šçš„ç ”ç©¶æŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š

## æ‰§è¡Œæ‘˜è¦
- æ ¸å¿ƒå‘ç°
- ä¸»è¦ç»“è®º

## è¯¦ç»†åˆ†æ
- å…³é”®æ´å¯Ÿ
- è¶‹åŠ¿åˆ†æ
- æŠ€æœ¯ç»†èŠ‚

## å®è·µå»ºè®®
- å¯è¡Œçš„è§£å†³æ–¹æ¡ˆ
- å®æ–½å»ºè®®
- æ½œåœ¨é£é™©

## è¿›ä¸€æ­¥ç ”ç©¶æ–¹å‘
- æœªè§£å†³çš„é—®é¢˜
- ç ”ç©¶ç©ºç™½
- æœªæ¥æœºä¼š

## å‚è€ƒæ–‡çŒ®æ€»ç»“
- å…³é”®æ–‡çŒ®åˆ†ç±»
- å¯ä¿¡åº¦è¯„ä¼°

è¯·ä½¿ç”¨ä¸“ä¸šçš„å­¦æœ¯è¯­è¨€ï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°ã€ç»“æ„å®Œæ•´ã€‚
"""

    try:
        logger.info("æ­£åœ¨ç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Š...")
        stream = client.chat.completions.create(
            model=ANALYSIS_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=ANALYSIS_MAX_TOKENS,
            stream=True
        )

        synthesis = ""
        reasoning = ""

        for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content or ""
                synthesis += content

                # å®æ—¶æ›´æ–°æ˜¾ç¤º
                if placeholder:
                    placeholder.markdown(synthesis + "â–Œ")

            if hasattr(chunk.choices[0].delta, 'reasoning_content'):
                reasoning += chunk.choices[0].delta.reasoning_content or ""

        # ç§»é™¤å…‰æ ‡
        if placeholder:
            placeholder.markdown(synthesis)

        logger.info(f"ç»¼åˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ - é•¿åº¦: {len(synthesis)} å­—ç¬¦ã€‚")
        return synthesis, reasoning
    except Exception as e:
        return f"ç»¼åˆåˆ†æå¤±è´¥: {str(e)}", ""


def save_research(filename, query, steps):
    """ä¿å­˜ç ”ç©¶ç»“æœ"""
    logger.info(f"ä¿å­˜ç ”ç©¶ç»“æœåˆ°æ–‡ä»¶: {filename}")
    research_data = {
        "query": query,
        "timestamp": datetime.now().isoformat(),
        "steps": []
    }

    for step in steps:
        research_data["steps"].append({
            "query": step.query,
            "step_type": step.step_type,
            "analysis": step.analysis,
            "reasoning": step.reasoning,
            "sources": step.sources,
            "timestamp": step.timestamp
        })

    filepath = os.path.join(RESEARCH_DIR, filename)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(research_data, f, ensure_ascii=False, indent=2)


def load_research(filename):
    """åŠ è½½ç ”ç©¶ç»“æœ"""
    filepath = os.path.join(RESEARCH_DIR, filename)
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)

    steps = []
    for step_data in data["steps"]:
        step = ResearchStep(
            query=step_data["query"],
            step_type=step_data["step_type"],
            sources=step_data.get("sources", []),
            analysis=step_data.get("analysis", ""),
            reasoning=step_data.get("reasoning", "")
        )
        steps.append(step)

    return data["query"], steps


# Streamlit ç•Œé¢
st.set_page_config(
    page_title="DeepResearch - æ·±åº¦ç ”ç©¶åŠ©æ‰‹",
    page_icon="ğŸ”¬",
    layout="wide"
)

init_research_session()

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.title("ğŸ”¬ DeepResearch")
    st.write("åŸºäºAIå’ŒExa APIçš„æ·±åº¦ç ”ç©¶å·¥å…·")

    st.subheader("ç ”ç©¶é…ç½®")
    st.session_state.research_depth = st.selectbox(
        "ç ”ç©¶æ·±åº¦",
        [1, 2, 3, 4, 5],
        index=2,
        help="æ¯ä¸ªæ·±åº¦çº§åˆ«ä¼šè¿›è¡Œæ›´æ·±å…¥çš„åˆ†æ"
    )

    st.session_state.max_sources_per_step = st.selectbox(
        "æ¯æ­¥æœ€å¤§æ¥æºæ•°",
        [3, 5, 7, 10],
        index=1,
        help="æ¯ä¸ªç ”ç©¶æ­¥éª¤æœç´¢çš„æœ€å¤§æ¥æºæ•°é‡"
    )

    st.subheader("å†å²ç ”ç©¶")
    research_files = [f for f in os.listdir(RESEARCH_DIR) if f.endswith('.json')]
    research_files.sort(reverse=True)

    for filename in research_files[:10]:
        if st.button(f"ğŸ“„ {filename[:-5]}", key=f"load_{filename}"):
            query, steps = load_research(filename)
            st.session_state.research_query = query
            st.session_state.research_steps = steps
            st.session_state.current_research = filename
            st.rerun()

# ä¸»ç•Œé¢
st.title("ğŸ”¬ DeepResearch - æ·±åº¦ç ”ç©¶åŠ©æ‰‹")
st.write("è¾“å…¥æ‚¨çš„ç ”ç©¶é—®é¢˜ï¼Œæˆ‘å°†è¿›è¡Œæ·±å…¥çš„å¤šæ­¥éª¤ç ”ç©¶åˆ†æ")

# ç ”ç©¶æŸ¥è¯¢è¾“å…¥
research_query = st.text_area(
    "ğŸ¯ ç ”ç©¶é—®é¢˜",
    value=st.session_state.research_query,
    height=100,
    placeholder="ä¾‹å¦‚ï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„åº”ç”¨ç°çŠ¶å’Œå‘å±•è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"
)

col1, col2, col3 = st.columns([1, 1, 2])
with col1:
    if st.button("ğŸš€ å¼€å§‹ç ”ç©¶", disabled=st.session_state.research_in_progress):
        if research_query.strip():
            st.session_state.research_query = research_query
            st.session_state.research_steps = []
            st.session_state.research_in_progress = True
            st.rerun()

with col2:
    if st.button("ğŸ”„ æ–°ç ”ç©¶"):
        st.session_state.research_query = ""
        st.session_state.research_steps = []
        st.session_state.current_research = None
        st.session_state.research_in_progress = False
        st.rerun()

# åœ¨æ‰§è¡Œæ·±åº¦ç ”ç©¶çš„éƒ¨åˆ†ï¼Œä¿®æ”¹è¿›åº¦è®¡ç®—é€»è¾‘
if st.session_state.research_in_progress and st.session_state.research_query:
    logger.info("=" * 50)
    logger.info(f"å¼€å§‹æ–°çš„ç ”ç©¶ä¼šè¯")
    logger.info(f"ç ”ç©¶é—®é¢˜: {st.session_state.research_query}")
    logger.info(f"ç ”ç©¶æ·±åº¦: {st.session_state.research_depth}")
    logger.info(f"æ¯æ­¥æœ€å¤§æ¥æºæ•°: {st.session_state.max_sources_per_step}")
    logger.info("=" * 50)

    # åˆ›å»ºå®æ—¶æ˜¾ç¤ºåŒºåŸŸ
    progress_container = st.container()
    process_container = st.container()

    with progress_container:
        progress_bar = st.progress(0)
        status_text = st.empty()
        current_time = st.empty()

    with process_container:
        st.subheader("ğŸ”„ ç ”ç©¶è¿›è¡Œä¸­...")
        step_containers = []

    try:
        # ä¿®å¤è¿›åº¦è®¡ç®—ï¼šä¸ºæ¯ä¸ªæ·±åº¦çº§åˆ«è®¡ç®—æŸ¥è¯¢æ•°é‡
        queries_per_depth = st.session_state.max_sources_per_step
        total_queries = st.session_state.research_depth * queries_per_depth
        total_steps = total_queries + 1  # +1 for synthesis
        current_step = 0
        existing_context = ""
        start_time = time.time()

        logger.info(
            f"æ€»æ­¥éª¤æ•°è®¡ç®—: {st.session_state.research_depth} æ·±åº¦ Ã— {queries_per_depth} æŸ¥è¯¢ + 1 ç»¼åˆ = {total_steps}")

        # å¤šè½®æ·±åº¦ç ”ç©¶
        for depth in range(st.session_state.research_depth):
            logger.info(f"å¼€å§‹ç¬¬ {depth + 1} è½®ç ”ç©¶ (æ·±åº¦çº§åˆ«: {depth + 1}/{st.session_state.research_depth})")

            # æ˜¾ç¤ºå½“å‰æ·±åº¦çº§åˆ«
            depth_container = process_container.container()
            with depth_container:
                st.markdown(f"### ğŸ” ç¬¬ {depth + 1} è½®æ·±åº¦ç ”ç©¶")
                depth_status = st.empty()
                depth_time = st.empty()

            step_start_time = time.time()

            # æ›´æ–°çŠ¶æ€å’Œæ—¶é—´
            elapsed_time = time.time() - start_time
            status_text.text(f"ğŸ” ç¬¬ {depth + 1} è½®æœç´¢ä¸­...")
            current_time.text(f"â±ï¸ å·²ç”¨æ—¶: {elapsed_time:.1f}ç§’")
            depth_status.text("ğŸ” ç”Ÿæˆæœç´¢æŸ¥è¯¢ä¸­...")

            # ç”Ÿæˆæœç´¢æŸ¥è¯¢
            queries = generate_search_queries(
                st.session_state.research_query,
                st.session_state.research_steps,
                depth + 1
            )

            # æ˜¾ç¤ºç”Ÿæˆçš„æŸ¥è¯¢
            with depth_container:
                st.write("**ğŸ“ ç”Ÿæˆçš„æœç´¢æŸ¥è¯¢:**")
                for i, query in enumerate(queries):
                    st.write(f"{i + 1}. {query}")

            for query_idx, query in enumerate(queries):
                step_start = time.time()

                # åˆ›å»ºæ­¥éª¤å®¹å™¨
                query_container = depth_container.container()
                with query_container:
                    st.markdown(f"#### æŸ¥è¯¢ {query_idx + 1}: {query}")
                    search_status = st.empty()
                    search_results = st.empty()
                    analysis_container = st.container()

                search_status.text("ğŸ” æœç´¢ä¸­...")

                # æœç´¢
                sources = search_with_exa(query, st.session_state.max_sources_per_step)

                # æ˜¾ç¤ºæœç´¢ç»“æœ
                with search_results:
                    if sources:
                        st.success(f"âœ… æ‰¾åˆ° {len(sources)} ä¸ªç›¸å…³æ¥æº")
                        with st.expander("æŸ¥çœ‹æœç´¢ç»“æœ"):
                            for i, source in enumerate(sources):
                                st.write(f"**{i + 1}.** [{source['title']}]({source['url']})")
                    else:
                        st.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ¥æº")

                # åˆ†æ
                search_status.text("ğŸ“Š åˆ†æä¸­...")

                with analysis_container:
                    st.write("**ğŸ“Š åˆ†æç»“æœ:**")
                    analysis_placeholder = st.empty()

                analysis, reasoning = analyze_sources_streaming(
                    query, sources, existing_context, analysis_placeholder
                )

                # æ›´æ–°è¿›åº¦ - ç¡®ä¿ä¸è¶…è¿‡1.0
                current_step += 1
                progress_value = min(current_step / total_steps, 0.95)  # æœ€å¤§95%ï¼Œä¸ºç»¼åˆåˆ†æç•™ç©ºé—´
                progress_bar.progress(progress_value)

                logger.info(f"è¿›åº¦æ›´æ–°: æ­¥éª¤ {current_step}/{total_steps}, è¿›åº¦å€¼: {progress_value:.3f}")

                elapsed_time = time.time() - start_time
                step_elapsed = time.time() - step_start
                current_time.text(f"â±ï¸ å·²ç”¨æ—¶: {elapsed_time:.1f}ç§’")
                search_status.text(f"âœ… å®Œæˆ (ç”¨æ—¶: {step_elapsed:.1f}ç§’)")

                # åˆ›å»ºç ”ç©¶æ­¥éª¤
                step = ResearchStep(
                    query=query,
                    step_type="search_analysis",
                    sources=sources,
                    analysis=analysis,
                    reasoning=reasoning
                )

                st.session_state.research_steps.append(step)
                existing_context += f"\n{query}: {analysis[:500]}...\n"

            # å®Œæˆè¯¥æ·±åº¦çº§åˆ«
            depth_elapsed = time.time() - step_start_time
            depth_time.text(f"â±ï¸ æœ¬è½®ç”¨æ—¶: {depth_elapsed:.1f}ç§’")

        # æœ€ç»ˆç»¼åˆ
        progress_bar.progress(0.96)  # è®¾ç½®ä¸º96%
        status_text.text("ğŸ¯ ç”Ÿæˆç»¼åˆæŠ¥å‘Šä¸­...")

        with process_container:
            st.markdown("### ğŸ¯ æœ€ç»ˆç»¼åˆåˆ†æ")
            synthesis_status = st.empty()
            synthesis_placeholder = st.empty()

        synthesis_status.text("ğŸ¯ ç»¼åˆåˆ†æä¸­...")

        final_synthesis, final_reasoning = synthesize_research_streaming(
            st.session_state.research_query,
            st.session_state.research_steps,
            synthesis_placeholder
        )

        synthesis_step = ResearchStep(
            query="æœ€ç»ˆç»¼åˆåˆ†æ",
            step_type="synthesis",
            sources=[],
            analysis=final_synthesis,
            reasoning=final_reasoning
        )
        st.session_state.research_steps.append(synthesis_step)

        # ä¿å­˜ç ”ç©¶
        timestamp = datetime.now().strftime("%m%d_%H%M")
        filename = f"{timestamp}_research.json"
        save_research(filename, st.session_state.research_query, st.session_state.research_steps)
        st.session_state.current_research = filename

        # å®Œæˆ
        total_time = time.time() - start_time
        progress_bar.progress(1.0)  # æœ€ç»ˆè®¾ç½®ä¸º100%
        status_text.text("âœ… ç ”ç©¶å®Œæˆï¼")
        current_time.text(f"â±ï¸ æ€»ç”¨æ—¶: {total_time:.1f}ç§’")
        synthesis_status.text(f"âœ… ç»¼åˆåˆ†æå®Œæˆ")

        logger.info("=" * 50)
        logger.info("ç ”ç©¶ä¼šè¯å®Œæˆ")
        logger.info(f"æ€»æ­¥éª¤æ•°: {len(st.session_state.research_steps)}")
        logger.info(f"ä¿å­˜æ–‡ä»¶: {filename}")
        logger.info("=" * 50)

        st.session_state.research_in_progress = False

        # æ˜¾ç¤ºå®Œæˆæ¶ˆæ¯
        st.success(f"ğŸ‰ ç ”ç©¶å®Œæˆï¼æ€»å…±ç”¨æ—¶ {total_time:.1f} ç§’ï¼Œå·²ä¿å­˜ä¸º {filename}")
        time.sleep(2)
        st.rerun()

    except Exception as e:
        logger.error(f"ç ”ç©¶è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}", exc_info=True)
        st.error(f"ç ”ç©¶è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        st.session_state.research_in_progress = False

# æ˜¾ç¤ºç ”ç©¶ç»“æœ
if st.session_state.research_steps:
    st.subheader("ğŸ¯ ç ”ç©¶é—®é¢˜")
    st.write(st.session_state.research_query)

    # æ˜¾ç¤ºæœ€ç»ˆç»¼åˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    synthesis_steps = [step for step in st.session_state.research_steps if step.step_type == "synthesis"]
    if synthesis_steps:
        st.subheader("ğŸ“‹ ç»¼åˆç ”ç©¶æŠ¥å‘Š")
        synthesis = synthesis_steps[-1]

        if synthesis.reasoning:
            with st.expander("ğŸ§  æ¨ç†è¿‡ç¨‹"):
                st.markdown(synthesis.reasoning)

        st.markdown(synthesis.analysis)

    # æ˜¾ç¤ºè¯¦ç»†ç ”ç©¶æ­¥éª¤
    st.subheader("ğŸ” è¯¦ç»†ç ”ç©¶è¿‡ç¨‹")

    search_steps = [step for step in st.session_state.research_steps if step.step_type == "search_analysis"]

    for i, step in enumerate(search_steps):
        with st.expander(f"æ­¥éª¤ {i + 1}: {step.query}"):
            col1, col2 = st.columns([2, 1])

            with col1:
                if step.reasoning:
                    st.write("**ğŸ§  æ¨ç†è¿‡ç¨‹:**")
                    st.markdown(step.reasoning[:500] + "..." if len(step.reasoning) > 500 else step.reasoning)

                st.write("**ğŸ“Š åˆ†æç»“æœ:**")
                st.markdown(step.analysis)

            with col2:
                st.write("**ğŸ“š å‚è€ƒæ¥æº:**")
                for j, source in enumerate(step.sources):
                    st.write(f"**æ¥æº {j + 1}:** [{source['title']}]({source['url']})")
                    if source.get('score'):
                        st.write(f"ç›¸å…³åº¦: {source['score']:.2f}")
                    st.write("---")

else:
    st.info("ğŸ‘† è¯·è¾“å…¥æ‚¨çš„ç ”ç©¶é—®é¢˜å¹¶ç‚¹å‡»'å¼€å§‹ç ”ç©¶'æ¥å¼€å§‹æ·±åº¦ç ”ç©¶")

# çŠ¶æ€æ˜¾ç¤º
if st.session_state.current_research:
    st.success(f"âœ… å½“å‰ç ”ç©¶å·²ä¿å­˜ä¸º: {st.session_state.current_research}")