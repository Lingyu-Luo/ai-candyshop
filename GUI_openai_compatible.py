import streamlit as st
from openai import OpenAI
import os
import json
import base64
from datetime import datetime
import re

client = OpenAI(
    api_key=os.getenv("GEMINI_API_KEY"),
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)
HISTORY_DIR = "ChatHistory"
os.makedirs(HISTORY_DIR, exist_ok=True)


def init_session():
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    if 'current_convo' not in st.session_state:
        st.session_state.current_convo = None
    if 'convo_list' not in st.session_state:
        st.session_state.convo_list = []
    if 'num_convo_display' not in st.session_state:
        st.session_state.num_convo_display = 10

def generate_filename(content):
    text_content = ""
    if isinstance(content, list):
        texts = [item["text"] for item in content if isinstance(item, dict) and item.get("type") == "text"]
        text_content = " ".join(texts)
    else:
        text_content = str(content)
    
    temp_messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå¯¹è¯å‘½ååŠ©æ‰‹ï¼Œå¸®åŠ©æå–å¯¹è¯å…³é”®è¯ä½œä¸ºå¯¹è¯è®°å½•æ–‡ä»¶åï¼Œåäº”å­—ä»¥å†…ã€‚"},
            {"role": "user", "content": "æå–å¯¹è¯çš„ä¸»é¢˜ï¼ˆä»…è¾“å‡ºä¸»é¢˜æœ¬èº«ï¼‰ï¼š" + text_content}
        ]
    response = client.chat.completions.create(
        model="gemini-3-flash-preview",
        #reasoning_effort="low",
        messages=temp_messages
    )
    
    clean_content = response.choices[0].message.content.strip().replace("\n", " ")
    clean_content = re.sub(r'[\n\r\t\\/*?:"<>|]', "", clean_content)[:15]
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"{timestamp}_{clean_content}.json" if clean_content else f"{timestamp}_æœªå‘½å.json"

def refresh_convo_list():
    st.session_state.convo_list = [
        f for f in os.listdir(HISTORY_DIR)
        if f.endswith('.json') and os.path.getsize(os.path.join(HISTORY_DIR, f)) > 0
    ]
    st.session_state.convo_list.reverse()
    
def new_conversation():
    st.session_state.messages = []
    st.session_state.current_convo = None


def load_conversation(filename):
    path = os.path.join(HISTORY_DIR, filename)
    with open(path, 'r', encoding='utf-8') as f:
        st.session_state.messages = json.load(f)
    st.session_state.current_convo = filename

def save_conversation():
    if st.session_state.current_convo and st.session_state.messages:
        path = os.path.join(HISTORY_DIR, st.session_state.current_convo)
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(st.session_state.messages, f, ensure_ascii=False, indent=2)

def render_with_latex(text: str):
    text = text.replace(r'\\\\',r"\\")
    text = text.replace(r'\(',r"$")
    text = text.replace(r'\)', r"$")
    text = text.replace(r'\[', r"$$")
    text = text.replace(r'\]', r"$$")
    st.markdown(text)

init_session()

with st.sidebar:
    st.title("å¯¹è¯ç®¡ç†")
    if st.button("â• æ–°å»ºå¯¹è¯", width='stretch'):
        st.session_state.messages = []
        st.session_state.current_convo = None
        st.rerun()
    refresh_convo_list()
    convo_render_list = st.session_state.convo_list[:st.session_state.num_convo_display]
    for convo in convo_render_list:
        cols = st.columns([3, 1])
        with cols[0]:
            if st.button(convo[:-5], key=f"btn_{convo}", width='stretch'):
                load_conversation(convo)
                st.rerun()
        with cols[1]:
            if st.button("Ã—", key=f"del_{convo}", type='primary'):
                os.remove(os.path.join(HISTORY_DIR, convo))
                if st.session_state.current_convo == convo:
                    new_conversation()
                st.rerun()
    if st.session_state.num_convo_display < len(st.session_state.convo_list):
        if st.button("åŠ è½½æ›´å¤š...",key="load_more_convo"):
            st.session_state.num_convo_display += 10
            st.rerun()


st.title("æ™ºèƒ½å¯¹è¯åŠ©æ‰‹ï¼ˆæ”¯æŒå›¾æ–‡ï¼‰")
for msg in st.session_state.messages:
    avatar = "ğŸ§‘" if msg["role"] == "user" else "ğŸ¤–"
    with st.chat_message(msg["role"], avatar=avatar):
        # å…ˆæ˜¾ç¤ºæ¨ç†å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
        if msg["role"] == "assistant" and msg.get("reasoning"):
            with st.expander("ğŸ§  æ¨ç†è¿‡ç¨‹ï¼ˆç‚¹å‡»å±•å¼€ï¼‰"):
                render_with_latex(msg["reasoning"])
        
        if isinstance(msg["content"], list):
            for item in msg["content"]:
                if item["type"] == "image_url":
                    try:
                        base64_str = item["image_url"]["url"].split(",")[1]
                        st.image(base64.b64decode(base64_str), width='stretch')
                    except:
                        st.error("å›¾ç‰‡åŠ è½½å¤±è´¥")
                elif item["type"] == "input_audio":
                    try:
                        audio_data = base64.b64decode(item["input_audio"]["data"])
                        st.audio(audio_data, format=f"audio/{item['input_audio']['format']}")
                    except:
                        st.error("éŸ³é¢‘åŠ è½½å¤±è´¥")
                elif item["type"] == "text" and item["text"].strip():
                    render_with_latex(item["text"])
        else:
            render_with_latex(msg["content"])


uploaded_files = st.file_uploader(
    "ğŸ“¤ ä¸Šä¼ å›¾ç‰‡æˆ–éŸ³é¢‘ï¼ˆæ”¯æŒå¤šé€‰ï¼‰",
    type=["png", "jpg", "jpeg", "wav", "mp3", "ogg", "flac", "aac", "m4a"],
    accept_multiple_files=True,
    key="file_uploader"
)

if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–æè¿°..."):
    # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯å†…å®¹
    message_content = []

    for uploaded_file in uploaded_files:
        if uploaded_file:
            base64_str = base64.b64encode(uploaded_file.read()).decode("utf-8")
            mime_type = uploaded_file.type
            
            # åˆ¤æ–­æ˜¯å›¾ç‰‡è¿˜æ˜¯éŸ³é¢‘
            if mime_type.startswith("image/"):
                message_content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:{mime_type};base64,{base64_str}"
                    }
                })
            elif mime_type.startswith("audio/"):
                # è·å–éŸ³é¢‘æ ¼å¼
                audio_format = uploaded_file.name.split(".")[-1].lower()
                message_content.append({
                    "type": "input_audio",
                    "input_audio": {
                        "data": base64_str,
                        "format": audio_format
                    }
                })
            uploaded_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ

    # å¤„ç†æ–‡æœ¬è¾“å…¥
    if prompt.strip():
        message_content.append({
            "type": "text",
            "text": prompt.strip()
        })
    
    user_message = {
        "role": "user",
        "content": message_content if len(message_content) > 1 else prompt
    }
    st.session_state.messages.append(user_message)
    
    with st.chat_message("user", avatar="ğŸ§‘"):
        for item in message_content:
            if item["type"] == "image_url":
                try:
                    base64_str = item["image_url"]["url"].split(",")[1]
                    st.image(base64.b64decode(base64_str), width='stretch')
                except:
                    st.error("å›¾ç‰‡æ˜¾ç¤ºå¤±è´¥")
            elif item["type"] == "input_audio":
                try:
                    audio_data = base64.b64decode(item["input_audio"]["data"])
                    st.audio(audio_data, format=f"audio/{item['input_audio']['format']}")
                except:
                    st.error("éŸ³é¢‘æ˜¾ç¤ºå¤±è´¥")
            elif item["type"] == "text":
                render_with_latex(item["text"])
    
    try:
        with ((st.chat_message("assistant", avatar="ğŸ¤–ï¸"))):
            reasoning_placeholder = st.empty()
            answer_placeholder = st.empty()
            full_reasoning = ""
            full_answer = ""
            
            response = client.chat.completions.create(
                model="gemini-3-flash-preview",
                #reasoning_effort="high",
                messages=st.session_state.messages,
                max_tokens=163840,
                stream=True
            )
            
            for chunk in response:
                print(chunk.model_dump())
                delta = chunk.choices[0].delta
                if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                    full_reasoning += delta.reasoning_content
                    reasoning_placeholder.markdown("ğŸ§  æ¨ç†è¿‡ç¨‹ï¼ˆç‚¹å‡»å±•å¼€ï¼‰")
                    with st.expander("ğŸ§  æ¨ç†è¿‡ç¨‹ï¼ˆç‚¹å‡»å±•å¼€ï¼‰"):
                        render_with_latex(full_reasoning)
                if delta.content:
                    full_answer += delta.content
                    answer_placeholder.markdown(full_answer)
            
            with reasoning_placeholder:
                if full_reasoning.strip():
                    with st.expander("ğŸ§  æ¨ç†è¿‡ç¨‹"):
                        render_with_latex(full_reasoning.strip())
            with answer_placeholder:
                render_with_latex(full_answer)
            st.session_state.messages.append({
                "role": "assistant",
                "content": full_answer,
                "reasoning": full_reasoning.strip()
            })
    
    except Exception as e:
        st.error(f"è¯·æ±‚å¤±è´¥: {str(e)}")
        st.session_state.messages.append({
            "role": "assistant",
            "content": "å“åº”ç”Ÿæˆå¤±è´¥",
            "reasoning": f"é”™è¯¯ä¿¡æ¯: {str(e)}"
        })
    
    filename_content = prompt.strip()
    if not st.session_state.current_convo:
        print("æ­£åœ¨ç”Ÿæˆå¯¹è¯æ–‡ä»¶å...")
        st.session_state.current_convo = generate_filename(filename_content)
    # ä¿å­˜å¯¹è¯è®°å½•
    if st.session_state.current_convo:
        save_conversation()
        refresh_convo_list()
    print("\n")
    st.rerun()

st.markdown("""
<script>
// è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
window.addEventListener('DOMContentLoaded', () => {
    const scrollToBottom = () => {
        window.scrollTo(0, document.body.scrollHeight);
    };
    scrollToBottom();
});
</script>
""", unsafe_allow_html=True)